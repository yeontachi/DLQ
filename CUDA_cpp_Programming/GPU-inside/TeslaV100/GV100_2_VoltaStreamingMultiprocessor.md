# VOLTA STREAMING MULTIPROCESSOR

## 🔹 Volta Streaming Multiprocessor (SM) 아키텍처

### 1. SM 구조 변화

* **Pascal GP100 SM**

  * 2개의 파티션
  * 각 파티션: FP32 코어 32개, FP64 코어 16개, 워프 스케줄러 1개, 디스패처 2개, 레지스터 파일 128KB
* **Volta GV100 SM**

  * 4개의 파티션
  * 각 파티션: FP32 코어 16개, FP64 코어 8개, **INT32 코어 16개**, **Tensor Core 2개**, 워프 스케줄러 1개, 디스패처 1개, 레지스터 파일 64KB, **L0 명령 캐시 추가**
* **효과**: FP/INT/Tensor 연산을 동시에 실행할 수 있어 SM 활용도가 향상

---

### 2. Tensor Core (혼합 정밀 연산)

* **딥러닝 행렬 연산 전용** 코어 도입
* Pascal 대비 **12배 빠른 학습 처리 성능** (동일 전력 기준)
* 하나의 Tensor Core는 **4×4 행렬 곱-누적 연산** 수행 (FP16 입력, FP32 누적)
* 딥러닝 학습, 특히 CNN·RNN·Transformer 등에 큰 성능 향상

---

### 3. 새로운 SIMT 모델

* 기존 SIMT: 한 워프(32스레드)가 항상 같은 명령을 실행 → 분기 시 성능 저하
* **Volta SIMT**: **독립 스레드 스케줄링** 지원

  * 워프 내부 스레드가 더 자유롭게 분기 및 재결합 가능
  * 일부 스레드가 메모리를 기다려도, 나머지는 계속 실행
* 불규칙한 워크로드(예: 조건문 많은 커널)에서도 효율 증가

---

### 4. 메모리·캐시 구조 개선

* **공유 메모리 + L1 캐시 통합**

  * Pascal: 최대 64KB → Volta: **96KB**
  * 필요에 따라 공유 메모리와 L1 캐시 비율을 설정 가능
* **L0 명령 캐시** (파티션별 추가) → Pascal의 instruction buffer보다 효율적
* **데이터 지역성 강화** → Stencil, Convolution, Reduction 같은 패턴에서 성능 상승

---

### 5. 프로그래밍 및 성능 영향

* **더 높은 동시성(Occupancy)**: 더 많은 레지스터와 SM 수 덕분에 더 많은 워프·스레드 동시 실행 가능
* **Tensor Core 활용**: CUDA WMMA API(`mma` intrinsic) 사용해야 함 → 행렬 곱, CNN, Transformer 학습 최적화
* **INT/FP 혼합 실행**: FP32 + INT32 연산을 동시에 실행 → 그래픽 + AI 연산 효율 상승
* **메모리 병렬화(Coalescing) 여전히 중요**: 전역 메모리는 여전히 느리므로, 연속 접근을 최적화해야 함

## Tensor Cores
### Tensor Cores와 Volta 아키텍처의 혁신
NVIDIA의 Tesla P100(Pascal 아키텍처 기반)은 이전 세대인 Maxwell과 Kepler GPU에 비해 신경망 학습에서 훨씬 더 높은 성능을 제공한다. 그러나 최근 인공지능 모델은 점점 더 복잡해지고 있다. 수천 개의 레이어와 수백만 개의 뉴런을 포함하는 초대형 신경망이 등장하면서, 단순히 기존 수준의 성능 향상만으로는 충분하지 않게 되었다. 이러한 대규모 신경망을 효과적으로 학습시키기 위해서는 **더 빠른 훈련 속도와 훨신 더 높은 연산 성능**이 필요하다.

이 요구를 충족시키기 위해 **Volta GV100 아키텍처** 에서는 **Tensor Core** 라는 새로운 연산 유닛이 도입되었다.
Tensor Core는 딥러닝 학습과 추론에 최적화된 특수한 연산 장치로, 대규모 행렬 곱(Matrix Multiplication, GEMM) 연산을 매우 빠르게 처리할 수 있다.

### Tesla V100의 Tensor Core 구성
Tesla V100 GPU에는 총 **640개의 Tensor Core** 가 탑재되어 있다.

각 SM(Streaming Multiprocessor)은 8개의 Tensor Core를 가지고 있으며, SM 내부의 각 **처리 파티션(Processing Block)** 마다 Tensor Core가 2개씩 배치된다.

이러한 구조 덕분에 병렬 연산 효율이 극대화 된다.

![alt text](/CUDA_cpp_Programming/GPU-inside/_Images/2_1.png)

Volta GV100에서 **각 Tensor Core는 클럭당 64개의 부동소수점 FMA(Fused Multiply-Add) 연산** 을 수행할 수 있다. 따라서 하나의 SM 안에 있는 8개의 Tensor Core는 클럭당 총 **512개의 FMA 연산** (즉, 1024개의 개별 부동소수점 연산)을 수행할 수 있다.

### 성능: TFLOPS의 도약
이 강력한 구조 덕분에 **Tesla V100의 Tensor Core**는 **최대 125 Tensor TFLOPS** 의 성능을 달성한다. 이는 단순히 FP32 연산만을 사용했던 Pascal P100과 비교했을 때 **최대 12배 더 높은 연산 성능** 을 제공한다.

 - **딥러닝 학습(Training)** 에서는 FP32 기반의 기존 방식 대비 12배 향상된 성능
 - **딥러닝 추론(Inference)** 에서는 FP16 기반 연산 대비 **최대 6배** 더 높은 성능

즉, 대규모 데이터셋과 복잡한 네트워크 구조에서도 학습 시간을 크게 단축할 수 있으며, 추론 역시 훨씬 더 빠르게 수행할 수 있다.

### 왜 Tensor Core인가?
신경망 학습과 추론의 핵심은 바로 **행렬-행렬 곱 연산(GEMM)**이다. 네트워크의 각 레이어에서는 입력 데이터와 가중치(Weights)가 대규모 행렬 형태로 곱해지며, 이 연산이 반복적으로 수행된다.

기존 GPU 아키텍처에서는 주로 FP32 단정밀도 연산을 사용했기 때문에 속도와 효율에 한계가 있었다. 이 한계를 해결하기 위해, Volta Tensor Core는 **FP16 입력을 받아 FP32로 누적(Accumulation)** 하는 **혼합 정밀(Mixed Precision)** 방식을 채택했다.

이 방식은 연산 속도를 비약적으로 높이면서도, 누적 과정에서 FP32 정밀도를 유지하므로 **정확도 저하 없이** 학습을 진행할 수 있었다.

실제 성능을 비교해보면,
**FP32 기반 행렬 곱** 에서는 P100 대비 **약 1.8배 성능 향상**, **FP16 입력 + FP32 누적 기반 행렬 곱** 에서는 **9배 이상의 성능 향상**을 달성했다.

![alt text](/CUDA_cpp_Programming/GPU-inside/_Images/2_2.png)

### Tensor Core의 동작
Tensor Core는 **행렬-곱셈 누적(Matrix Multiply and Accumulate, MMA)** 연산을 위한 전용 하드웨어이다.
각 Tensor Core는 **4x4 크기의 행렬 연산** 을 단위로 하여 다음과 같은 연산을 수행한다.

$$
D = A \times B + C
$$

 - $A,B$ : 입력 행렬 (4x4, FP16 형식)
 - $C,D$ : 누적 및 결과 행렬 (4x4, FP16 또는 FP32 형식)

즉, Tensor Core는 두 개의 FP16 행렬을 곱하고, FP16 또는 FP32 형식의 행렬 $C$에 더해 최종적으로 결과 행렬 $D$를 만들어 낸다.

![alt text](/CUDA_cpp_Programming/GPU-inside/_Images/2_3.png)

### 혼합 정밀(Mixed Precision) 연산
Tensor Core는 **FP16 입력 데잍처**를 받아 연산을 수행하되, 결과를 **FP32 누적기(accumulator)**를 통해 합산하는 방식을 사용한다.

먼저, 두 FP16 값이 곱해지면서 단순히 FP16 정밀도의 결과가 아니라 **Full Precision 곱셈 결과** 가 생성된다. 이렇게 얻어진 곱셈 결과는 FP32 형식으로 변환되어 누적기에 저장된다. 이후 다른 곱셈 결과들과 함께 **FP32 덧셈** 으로 합산되면서 최종적으로 정확한 결과가 유지된다.

![alt text](/CUDA_cpp_Programming/GPU-inside/_Images/2_4.png)

즉, 한 번의 4x4x4 행렬 곱 연산 과정에서, 모든 곱셈은 FP16으로 빠르게 수행되지만, 그 결과는 FP32 누적기를 통해 더해지기 때문에 **속도와 정확도를 동시에 확보** 할 수 있다.

하지만 실제 응용에서는 Tensor Core가 단순히 작은 4x4 행렬 곱셈만 수행하는 것이 아니다. 
Tensor Core는 이러한 **작은 블록 연산(4x4x4)** 을 기본 단위로 사용하고, 이 블록들을 결합하여 훨씬 더 큰 2차원 행렬 연산 또는 고차원 행렬 연산을 구성한다.

따라서 대규모 신경망 학습이나 과학 계산에서 필요한 거대한 행렬 곱도, 결국은 Tensor Core가 반복적으로 실행하는 작은 4x4 연산들이 쌓여 이루어지는 것이다.

### Pascal과 Volta Tensor Core의 4x4 행렬 곱 비교
행렬 곱셈에서 4x4 크기의 두 입력 행렬이 주어지면, 출력 행렬의 각 원소를 계산하기 위해 **64번의 곱셈 및 덧셈 연산**이 필요하다. 아래 그림에서는 이 64개의 연산은 큐브 형태로 표현되어 있으며, 큐브 위쪽에 두 개의 4x4 입력 행렬이 놓이고, 그 결과로 아래쪽에 하나의 4x4 출력이 생성된다.

![alt text](/CUDA_cpp_Programming/GPU-inside/_Images/2_5.png)

**Pascal(Tesla P100)** 의 처리 방식을 보면,
4x4 행렬 곱을 수행하기 위해 각 곱셈과 덧셈을 **순차적**으로 계산해야 한다. 즉, 64개의 연산이 그대로 필요한 셈이며, 이는 연산 병렬화 측면에서 한계가 존재한다.

반면, **Volta** 아키텍처의 V100 GPU는 **Tensor Core**라는 전용 연산 유닛을 통해 이 과정을 크게 가속한다.
Tensor Core는 4x4 행렬 곱-누적(Matrix Multiply and Accumulate, MMA) 연산을 하드웨어 차원에서 처리하도록 설계되어 있다. 그 결과, 같은 64개의 연산을 Pascal보다 훨씬 더 빠르게 처리할 수 있으며, 실제로는 **Pascal 대비 약 12배 높은 연산 속도** 를 제공한다.

### Volta 아키텍터와 Tensor Core 프로그래밍
Volta 아키텍처에서 새롭게 도입된 Tensor Core는 단순히 하드웨어 가속 장치로만 머무는 것이 아니라, 프로그래머가 직접 활용할 수 있도록 CUDA 9 C++ API를 통해 공개되었다. 이 API는 행렬 연산을 효율적으로 다루기 위해 특별히 설계되었으며, 행렬 불러오기(matrix load), 행렬 곱과 누적(matrix multiply and accumulate), 행렬 저장(matrix store)이라는 세 가지 핵심 연산을 지원한다. 이러한 연산 흐름을 통해 사용자는 기존의 CUDA 프로그램에 Tensor Core 연산을 자연스럽게 통합할 수 있다.

Tensor Core는 워프 단위에서 동작한다는 점이 특징적이다. 하나의 워프는 32개의 스레드로 구성되는데, CUDA 레벨에서 이 워프는 16×16 크기의 행렬 연산을 처리하도록 추상화되어 있다. 내부적으로는 여러 개의 4×4 Tensor Core 블록 연산이 결합되어 있지만, 프로그래머는 이를 직접 의식할 필요가 없다. CUDA는 이를 하나의 큰 행렬 연산 단위로 제공하기 때문에, 사용자는 복잡한 하드웨어 구조를 신경 쓰지 않고도 고성능의 행렬 연산을 수행할 수 있다.

NVIDIA는 Tensor Core 활용성을 더욱 높이기 위해 자사의 핵심 라이브러리인 cuBLAS와 cuDNN을 업데이트하였다. cuBLAS는 행렬 연산 최적화 라이브러리이고, cuDNN은 딥러닝 연산을 위한 라이브러리로, 두 라이브러리 모두 Tensor Core 연산을 지원한다. 덕분에 개발자는 복잡한 CUDA API를 직접 다루지 않더라도, 단순히 라이브러리 호출만으로 Tensor Core의 성능을 활용할 수 있다.

이와 더불어 NVIDIA는 주요 딥러닝 프레임워크와 협력하여 Tensor Core 지원을 확대하고 있다. 이미 Caffe2와 MXNet 같은 프레임워크에서는 Volta GPU의 Tensor Core를 활용할 수 있으며, 앞으로 더 많은 프레임워크가 Tensor Core 최적화를 지원할 예정이다. 이를 통해 연구자와 개발자는 별도의 구현 부담 없이도 신경망 학습과 추론에서 Tensor Core의 성능 향상을 즉시 누릴 수 있다.

요약하자면, Tensor Core는 CUDA 9 C++ API를 통한 저수준 프로그래밍, cuBLAS·cuDNN 라이브러리 차원의 고수준 최적화, 그리고 딥러닝 프레임워크 통합이라는 세 가지 층위에서 접근할 수 있다. 이러한 다층적 지원 덕분에 Volta 아키텍처는 단순한 하드웨어 성능 향상을 넘어, 실제 개발 환경에서의 생산성까지 고려한 완성도 높은 GPU 플랫폼으로 자리잡게 되었다.

## Enhanced L1 Data Cache and Shared Memory
### Volta의 L1 데이터 캐시와 공유 메모리 통합 설계
Volta 아키텍처의 SM(Streaming Multiprocessor)에는 새로운 **L1 데이터 캐시와 공유 메모리 통합 서브 시스템**이 도입되었다. 이 설계는 성능을 크게 향상시키는 동시에 프로그래밍을 단순화하며, 최적 성능에 도달하기 위해 필요한 튜닝 과정도 줄여준다.

기존 GPU에서는 데이터 캐시와 공유 메모리가 별도로 존재했지만, Volta에서는 이 두기능을 **하나의 메모리 블록으로 통합** 하였다. 그 결과, 어떤 프로그램이 공유 메모리를 사용하지 않는다면 전체 용량을 데이터 캐시로 활용할 수 있게 되었고, 반대로 공유 메모리를 필요로 하는 경우에도 유연하게 설정할 수 있다. 전체 용량은 **SM당 128KB**로, Pascal GP100의 데이터 캐시에 비해 무려 7배 이상 커졌다. 또한 텍스처 유닛 역시 이 캐시를 사용할 수 있다. 예를 들어, 공유 메모리를 64KB로 설정하면 나머지 64KB는 L1 캐시로 활용되어 텍스처 연산 및 로드/스토어 연산에서 사용된다.

### 낮은 지연시간과 높은 대역폭
공유 메모리 블록 내부에 L1 캐시를 통합한 것은 성능 측면에서 중요한 의미를 가진다. Volta GV100의 L1 캐시는 이전 세대 GPU의 L1 캐시에 비해 **훨씬 낮은 지연시간**과 **더 높은 대역폭** 을 제공한다. 따라서 Volta의 L1은 단순히 스트리밍 데이터를 위한 고속 경로 역할 뿐 아니라, 자주 재사용되는 데이터를 빠르게 접근할 수 있는 저지연 메모리로도 활용된다. 결과적으로 두 가지 장점을 모두 갖춘 "하이브리드 메모리" 역할을 수행한다.

### 공유 메모리와 L1 캐시의 간극을 줄이다.
Volta에서 L1과 공유 메모리를 통합한 또 다른 핵심 이유는, L1 캐시 연산이 **공유 메모리의 성능적 장점**을 최대한 누리도록 하기 위함이다. 공유 메모리는 높은 대역폭, 낮은 지연시간, 그리고 캐시 미스가 없는 일관된 성능을 제공하지만, CUDA 프로그래머가 이를 명시적으로 관리해야 한다는 부담이 있었다. 반면 L1 캐시는 프로그래머가 별도로 관리하지 않아도 되지만, 캐시 미스로 인한 성능 변동이 존재했다.

Volta의 새로운 설계는 이 두 메모리 모델 사이의 간극을 줄인다. 실제로 NVIDIA는 여러 프로그램을 대상으로 공유 메모리 배열을 디바이스 메모리 배열로 바꾸어 L1 캐시를 통해 접근하도록 실험했다. 그 결과 Volta에서는 공유 메모리를 사용하지 않았을 때 성능 손실이 약 **7%** 에 불과했으며, Pascal에서는 동일한 상황에서 **30% 손실** 이 발생했다.

![alt text](/CUDA_cpp_Programming/GPU-inside/_Images/2_6.png)

### 프로그래밍 단순화와 성능 확보
이 결과는 중요한 의미를 가진다. 여전히 공유 메모리가 최대 성능을 내기 위한 최적의 선택지임은 변함없지만, Volta에서는 굳이 공유 메모리를 정교하게 관리하지 않아도 **우수한 성능을 빠르게 확보** 할 수 있게 된 것이다. 즉, 프로그래머 입장에서는 **개발 난이도를 낮추면서도 높은 성능을 얻을 수 있는 환경** 이 마련된 셈이다.

### Volta GV100의 L1 캐시와 성능 향상
Volta GV100의 L1 캐시는 **공유 메모리가 최적의 선택이 아니거나 사용할 수 없는 상황에서도 성능을 크게 향상**시킬 수 있도록 설계되었다. 이전 세대 GPU에서 공유 메모리는 뛰어난 성능을 제공했지만, 항상 모든 프로그램에 적합한 것은 아니었다. 예를 들어, 특정 패턴의 데이터 접근이나 동적으로 크기가 변하는 데이터 구조에서는 공유 메모리를 활용하기 어려웠다. 이러한 경우, GV100의 새로운 L1 캐시 구조는 중요한 대안이 된다.

Volta에서 공유 메모리와 L1 캐시를 통합한 결과, 글로벌 메모리로의 경로는 더욱 **고속(High-speed)**으로 변했다. 이 통합 구조는 무제한의 캐시 미스를 동시에 처리할 수 있는 **스트리밍 접근(streaming access)**을 지원한다. 이는 많은 스레드가 동시에 데이터를 요청하더라도 병목 현상을 최소화할 수 있음을 의미한다.

또한, GV100의 또 다른 혁신은 **쓰기 캐싱(write-caching)** 기능의 도입이다. 과거의 NVIDIA GPU들은 **로드 캐싱(load caching)**, 즉 읽기 연산을 위한 캐싱만 지원했다. 따라서 쓰기 연산은 항상 상대적으로 느린 글로벌 메모리에 직접 반영되어야 했고, 이는 성능 저하의 원인이 되었다. 그러나 GV100에서는 쓰기 연산(store operations)도 캐싱할 수 있어, 읽기뿐 아니라 쓰기에서도 대역폭 활용 효율과 실행 속도가 크게 개선되었다.

즉, GV100의 L1 캐시는
 - **공유 메모리 사용이 어려운 경우에도 성능 향상**을 제공하며,
 - **무제한 캐시 미스 동시 처리** 로 대규모 병렬 실행 환경에 최적화되어 있고,
 - **쓰기 캐싱(write-caching) 지원**으로 이전 세대 GPU 대비 한층 더 높은 메모리 성능을 보장한다.

## Simultaneous Execution of FP32 and INT32 Operations
### Volta에서의 FP32와 INT32 연산 동시 실행
Pascal 아키텍처의 GPU에서는 FP32(단정밀도 부동소수점)와 INT32(정수) 명령을 동시에 실행할 수 없었다. 따라서 부동 소수점 연산과 정수 연산이 함께 필요한 코드에서는 두 연산이 순차적으로 실행되어야 했고, 이로 인해 파이프라인의 활용도가 떨어지고 전체 성능이 제한되었다.

Volta GV100 SM은 이러한 제약을 해결하기 위해 FP32 연산 코어와 INT32 연산 코어를 **분리된 전용 유닛**으로 배치하였다. 덕분에 Volta는 **FP32 연산과 INT32 연산을 동시에 전속도(full throughput)**로 실행할 수 있으며, 그 결과 명령 발행 처리량(Intruction issue throughput)이 크게 증가하였다.

또한 핵심적인 FMA(Fused Multiply-Add) 연산의 지연(latency)도 개선되었다. Pascal에서는 FMA 연산을 완료하는데 6클럭 사이클이 필요했지만, Volta에서는 이를 **4클럭 사이클** 로 줄였다. 이러한 개선은 수치 계산이 많은 과학 계산이나 머신 러닝 애플리케이션에서 곧바로 성능 향상으로 이어진다.

### 실제 응용에서의 이점
많은 애플리케이션의 내부 루프(Inner loop)에서는 부동소수점 연산과 함께 **포인터 산술(pointer arithmetic)**이 동시에 수행된다. 예를 들어, 배열 인덱스를 계산하거나 메모리 주소를 갱신하는 과정은 정수 연산(INT32)을 필요로 하고, 동시에 데이터에 대한 수치 계산은 FP32 연산으로 처리된다.

Pascal에서는 이 두연산이 분리되어 순차적으로 처리되었기 때문에 효율이 떨어졌다. 그러나 Volta에서는 **각 루프 반복(iteration)** 이 다음과 같이 병렬로 실행될 수 있다.

 - INT32 코어가 다음 반복에서 사용할 메모리 주소를 계산하는 동안, FP32 코어는 현재 반복의 계산을 처리한다.

즉, 루프의 각 반복에서 **주소 갱신과 데이터 로드(INT32)**, **수치 연산(FP32)** 이 동시에 진행됨으로써, 파이프라인의 활용도가 극대화되고 전체 실행 속도가 향상된다.
