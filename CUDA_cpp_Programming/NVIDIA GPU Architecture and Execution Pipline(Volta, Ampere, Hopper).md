# NVIDIA GPU Architecture and Execution Pipeline(Volta, Ampere, Hopper)

### 현대 NVIDIA GPU 아키텍처 개요

NVIDIA의 **Volta, Ampere, Hopper 아키텍처**는 현대의 딥러닝과 고성능 컴퓨팅(HPC)을 이끄는 세 가지 세대를 대표합니다. 이들은 공통적으로 **대규모 병렬 연산 성능**을 극대화하면서도, 세대가 바뀔 때마다 새로운 기능과 최적화 요소를 도입하여 GPU 활용 범위를 크게 확장시켰습니다.

먼저 **Volta 아키텍처**는 2017년에 등장한 **Tesla V100** GPU를 통해 세상에 처음 공개되었습니다. Volta의 가장 큰 특징은 **Tensor Core의 최초 도입**입니다. Tensor Core는 혼합 정밀도(mixed-precision) 행렬 연산을 가속하는 하드웨어 유닛으로, FP16 연산을 빠르게 수행하면서 FP32 정밀도로 누적을 지원합니다. 또한 Volta는 **독립 스레드 스케줄링(Independent Thread Scheduling)** 모델을 도입하여, 워프 내 스레드가 각각 독립적인 프로그램 카운터를 가지도록 개선했습니다. 이 두 가지 혁신은 딥러닝 학습 성능을 크게 끌어올렸으며, Volta를 본격적인 AI 시대의 GPU 아키텍처로 자리매김하게 만들었습니다.

다음으로 **Ampere 아키텍처**는 2020년에 공개된 **A100 GPU**를 대표 모델로 합니다. Ampere는 7nm 공정에서 540억 개의 트랜지스터를 집적하여, **연산 및 메모리 성능을 거의 두 배**로 끌어올렸습니다. 이 세대에서 주목할 만한 기능은 **Multi-Instance GPU(MIG)**입니다. 이를 통해 하나의 GPU를 최대 7개의 독립된 인스턴스로 나누어 사용할 수 있어, 데이터센터 환경에서 GPU 자원의 활용도를 크게 높였습니다. 또한 3세대 Tensor Core는 **TF32, BF16, INT8/4, 구조적 희소성(Structured Sparsity)**을 지원하며, 이를 통해 AI 워크로드에서 최대 20배에 달하는 성능 향상을 보여주었습니다. 여기에 더해, L2 캐시는 6MB(V100)에서 무려 40MB로 확대되어 대규모 데이터 처리 효율성을 크게 향상시켰습니다.

마지막으로 **Hopper 아키텍처**는 2022년 출시된 **H100 GPU**로 구현되었습니다. Hopper는 TSMC 4N 공정을 사용해 800억 개의 트랜지스터를 집적하며, 이전 세대 대비 훨씬 더 많은 **SM(Streaming Multiprocessor)과 연산 유닛**을 제공합니다. 이 아키텍처에서는 **4세대 Tensor Core**가 도입되었으며, FP8 정밀도를 지원하여 더 작은 메모리 풋프린트와 더 빠른 연산 속도를 제공합니다. 또한, **DPX 명령어**를 새롭게 도입하여 유전체 분석이나 동적 프로그래밍(dynamic programming)과 같은 특정 알고리즘을 대폭 가속화합니다. 여기에 50MB L2 캐시, HBM3 메모리, NVLink 4 인터커넥트와 같은 아키텍처적 개선이 더해져, **행렬 연산은 약 6배, 대규모 언어 모델 학습은 최대 9배, 추론은 최대 30배** 빠른 성능을 A100 대비 제공할 수 있게 되었습니다.

이처럼 Volta → Ampere → Hopper로 이어지는 아키텍처 발전은, 단순히 연산 속도의 향상만이 아니라 **AI와 HPC를 위한 특화된 기능 추가**에 초점을 맞추고 있습니다. Tensor Core의 세대별 발전, 메모리 계층 구조의 확장, 그리고 GPU 자원의 가상화 기능은 현대 딥러닝과 양자화 연구에 있어서 필수적으로 이해해야 할 핵심 요소입니다.

### 스트리밍 멀티프로세서(SM), CUDA 코어, Tensor 코어

NVIDIA GPU의 기본 연산 단위는 **스트리밍 멀티프로세서(Streaming Multiprocessor, SM)**입니다. SM은 일종의 고도로 병렬화된 "코어"로 볼 수 있으며, 내부에는 다양한 연산 유닛이 포함되어 있습니다. 대표적으로 정수 및 부동소수점 연산을 담당하는 **CUDA 코어(ALU)**, 그리고 행렬 곱셈-누적(matrix multiply-accumulate)을 가속하는 **Tensor 코어**, 여기에 각 스레드가 사용할 수 있는 **레지스터 파일과 캐시 메모리**가 있습니다.

세대가 발전할수록 SM의 성능은 크게 향상되었습니다. 예를 들어, **Volta 아키텍처**의 SM은 64개의 단정밀도(FP32) 코어, 32개의 배정밀도(FP64) 코어, 그리고 8개의 1세대 Tensor 코어를 포함합니다. **Ampere 아키텍처**의 SM은 이와 유사한 구성을 유지하면서 Tensor 코어의 성능을 크게 개선했습니다. Ampere의 SM에는 Tensor 코어가 4개 있지만, 각 코어가 이전보다 4배 더 많은 FP16/FP32 연산을 처리할 수 있어, SM 하나당 클럭당 최대 1024개의 FMA 연산을 수행할 수 있습니다. 한편, **Hopper 아키텍처**는 FP 연산 유닛 수를 두 배로 늘려, SM 하나에 128개의 FP32 코어와 64개의 FP64 코어, 그리고 4개의 Tensor 코어를 배치했습니다. 이를 통해 AI와 HPC(고성능 컴퓨팅) 워크로드 모두에서 처리량을 크게 높였습니다.

다음 그림은 **Ampere SM 아키텍처**의 구성을 보여줍니다. SM은 네 개의 처리 블록으로 나뉘며, 각 블록에는 **워프 스케줄러(warp scheduler)**, **디스패치 유닛**, 16개의 FP32 코어, 16개의 INT32 코어, 8개의 FP64 연산 유닛, 그리고 하나의 Tensor 코어가 포함되어 있습니다. 이 모든 블록은 128KB 레지스터 파일과 공유 메모리/L1 캐시를 공유합니다.

비록 세대별 세부 구성이 다르더라도, 모든 SM은 **SIMT(Single-Instruction, Multiple-Thread)** 실행 모델을 따릅니다. SIMT는 하나의 워프(32개의 병렬 스레드 그룹)가 동일한 명령을 동시에 실행하면서도, 각 스레드가 독립적인 데이터를 처리하는 방식입니다. 이는 32폭 벡터 유닛과 유사하지만, 각 스레드가 고유한 레지스터를 가지며 독립적으로 분기할 수 있다는 점에서 차이가 있습니다.

특히 **Volta 아키텍처**는 **독립 스레드 스케줄링(Independent Thread Scheduling)**을 처음 도입했습니다. 이 모델에서 워프 내 각 스레드는 자신만의 프로그램 카운터와 호출 스택(call stack)을 가집니다. 따라서 분기문에서 스레드들이 서로 다른 경로를 따르더라도, 더 이상 전체 워프가 lockstep 방식으로 묶여 실행되지 않습니다. 대신 스레드별로 독립적으로 스케줄링되거나 일시 중지될 수 있으며, 필요한 경우 `__syncwarp()`와 같은 새로운 명령어를 통해 프로그래머가 명시적으로 동기화를 제어할 수 있습니다.

이러한 유연성은 불규칙한 제어 흐름을 가지는 프로그램에서 GPU의 효율성을 높여주지만, 동시에 올바른 결과를 위해서는 CUDA 코드에서 적절한 **동기화 프리미티브**를 반드시 사용해야 한다는 점을 의미합니다.

### 메모리 계층 구조와 GPU 인터커넥트

NVIDIA GPU는 수천 개의 연산 코어에 데이터를 안정적으로 공급하기 위해 **계층적 메모리 시스템(hierarchical memory system)**을 사용합니다. 각 SM(Streaming Multiprocessor)에는 고속의 온칩 메모리가 탑재되어 있는데, 이 메모리는 **L1 캐시와 공유 메모리(shared memory)**라는 두 가지 역할을 동시에 수행할 수 있습니다. Volta 세대에서는 SM당 128KB, Ampere 세대에서는 192KB, Hopper 세대에서는 256KB로 점차 확장되었습니다. CUDA 프로그래밍에서는 이 메모리를 공유 메모리로 직접 활용할 수 있어, 동일한 블록 내 스레드들이 낮은 지연(latency)으로 데이터를 주고받으며 협력할 수 있습니다.

또한 각 SM은 대규모 레지스터 파일을 보유하고 있습니다. Volta, Ampere, Hopper 세대 모두 SM당 **65,536개의 32비트 레지스터**를 갖추고 있으며, 이를 통해 각 스레드는 수십 개의 레지스터를 빠른 로컬 저장소로 활용할 수 있습니다.

모든 SM은 칩 위에 위치한 **공통 L2 캐시**에 연결됩니다. 이 L2 캐시는 상위 메모리(HBM)와의 데이터 교환을 완충하는 역할을 합니다. NVIDIA는 최근 세대에서 L2 캐시 용량을 크게 확대했는데, V100(Volta)은 6MB, A100(Ampere)은 40MB, H100(Hopper)은 50MB를 지원합니다. 이를 통해 모델 파라미터나 활성화 값과 같은 데이터셋을 더 많이 칩 내부에 보관할 수 있어, 빈번한 외부 메모리 접근을 줄일 수 있습니다.

데이터센터용 GPU의 메인 메모리는 **HBM(High Bandwidth Memory)**입니다. 예를 들어 A100은 40GB의 HBM2e 메모리를 탑재하고 최대 **1.5TB/s 대역폭**을 제공하며, H100은 80~96GB의 HBM3를 사용하여 **최대 3TB/s 대역폭**을 지원합니다. 이러한 방대한 메모리 대역폭은 대규모 텐서를 지속적으로 스트리밍해야 하는 딥러닝 워크로드에서 필수적입니다.

여러 GPU 간의 확장을 위해 NVIDIA는 **NVLink**와 **NVSwitch** 같은 고속 인터커넥트를 사용합니다. Volta는 **NVLink 2.0**을 도입하여 GPU 간 약 300GB/s의 대역폭을 지원했고, Ampere는 이를 두 배로 늘린 **NVLink 3.0(600GB/s)**를, Hopper는 다시 향상된 **NVLink 4.0(900GB/s)**를 제공합니다. 이를 통해 멀티 GPU 학습이나 데이터 병렬 처리에서 **all-reduce 연산**과 같은 집약적인 통신 작업을 빠르게 수행할 수 있습니다. 또한 DGX와 같은 서버 시스템에서는 **NVSwitch 칩**을 이용해 8개 이상의 GPU 간에 **모든-모든(all-to-all)** 고속 연결을 지원합니다.

이러한 대역폭 높은 메모리 계층과 빠른 GPU 간 인터커넥트는 각 SM의 실행 유닛이 항상 충분한 데이터를 공급받도록 보장하며, 결과적으로 GPU의 연산 처리량(throughput)을 극대화하는 핵심 요소로 작동합니다.

### 실행 파이프라인: 워프, 스케줄링, 그리고 지연 은닉

GPU의 높은 처리량(throughput)을 가능하게 하는 핵심은 **워프(warp)** 단위의 스레드들을 효율적으로 관리하고 실행하는 능력입니다. 각 SM(Streaming Multiprocessor)에는 여러 개의 **워프 스케줄러(warp scheduler)**가 존재하며, 이들은 준비된 워프들로부터 명령어를 가져와 사이클 단위로 발행합니다. 이러한 방식은 라운드 로빈(round-robin)처럼 세밀하게 동작하여, 항상 하드웨어가 바쁘게 움직이도록 보장합니다.

예를 들어, Volta와 Ampere 아키텍처의 경우 각 SM에는 4개의 워프 스케줄러가 있으며, 각 스케줄러는 사이클마다 준비된 워프로부터 하나의 명령어를 발행할 수 있습니다. 또한 상황에 따라 서로 다른 파이프라인으로 **듀얼 발행(dual-issue)**도 가능하여, 병렬성을 한층 강화합니다. Volta에서는 일반 산술 연산의 파이프라인 지연이 기존 Pascal 세대의 6사이클에서 4사이클로 줄어들었기 때문에, SM당 단 4개의 워프만으로도 연산 지연을 완전히 은닉할 수 있었습니다. 하지만 실제로는 메모리 접근 지연이 수백 사이클에 이르기 때문에, 최대 64개의 활성 워프(즉, 2048개의 스레드)를 동시에 유지하여 연산과 메모리 병목을 함께 가려줍니다. 이처럼 **제로 오버헤드 스레드 스위칭**은 GPU 설계의 핵심으로, 막대한 수의 병렬 스레드를 이용해 지연(latency)을 사실상 보이지 않게 만드는 것입니다.

실행 파이프라인은 기본적으로 **SIMT(Single Instruction, Multiple Threads)** 모델을 따릅니다. 즉, 하나의 워프는 동일한 명령어를 동시에 실행하지만, 각 스레드는 고유한 데이터를 처리합니다. 만약 워프 내 스레드들이 분기문에서 갈라진다면(if/else), 워프는 각 경로를 직렬로 실행하면서 해당하지 않는 스레드를 마스킹 처리합니다. Volta에서 도입된 **독립 스레드 스케줄링(Independent Thread Scheduling)**은 이러한 분기 상황을 더 유연하게 다룰 수 있도록, 각 스레드가 개별적인 프로그램 카운터와 스택을 가지도록 했습니다. 이 덕분에 이전 세대처럼 워프 전체가 반드시 동시에 재수렴할 필요가 없게 되었지만, 올바른 실행을 위해서는 `__syncwarp()` 같은 워프 단위 동기화를 명시적으로 사용해야 합니다.

Ampere 아키텍처는 명령 발행 하드웨어를 더욱 개선했습니다. 예컨대 Ampere 기반 GA102 GPU(GeForce RTX 30 시리즈에 사용)에서는 각 SM 파티션의 두 데이터 경로가 모두 FP32 연산을 처리할 수 있게 되어, 이전 세대에서는 하나가 INT32 전용이었던 것과 달리 **FP32 처리량이 두 배**로 늘어났습니다. 이는 워프 스케줄러가 하나의 워프에 대해 동일 사이클 내에 FP32 연산 명령을 두 경로에 동시에 발행할 수 있게 만들었고, 결과적으로 수학 연산의 처리량을 크게 끌어올렸습니다. Hopper 아키텍처에서는 이보다 한층 더 개선된 스케줄러와 데이터 경로를 통해 더 높은 병렬성을 유지합니다. 또한 새로운 **DPX 유닛**이 추가되어 동적 프로그래밍(dynamic programming) 같은 복잡한 알고리즘을 가속화할 수 있게 되었습니다. 즉, 세대가 발전할수록 GPU는 연산 유닛을 최대한 바쁘게 유지하기 위해 **스톨(stall)을 회피하고, 연산과 데이터 이동을 중첩 실행**하는 방식으로 진화하고 있습니다.

메모리 연산은 **로드/스토어 유닛**이 담당하며, 이는 산술 파이프라인과 동시에 동작할 수 있습니다. Ampere는 여기에 **비동기 데이터 전송 명령어(`MemcpyAsync`)**를 도입하여, 워프가 글로벌 메모리에서 공유 메모리로 데이터를 옮길 때 다른 스레드를 멈추지 않고도 수행할 수 있도록 했습니다. 여기에 새로운 비동기 배리어 메커니즘이 더해져, 계산과 데이터 전송을 더욱 효율적으로 겹쳐 실행할 수 있게 되었고, 이는 지연 은닉과 메모리 대역폭 활용도를 극대화하는 데 크게 기여했습니다.

정리하자면, GPU의 실행 파이프라인은 수만 개의 스레드와 수천 개의 코어가 만들어내는 막대한 병렬성에 기반합니다. 워프 스케줄러는 빠른 문맥 전환과 정교한 명령 발행을 통해 연산 지연과 메모리 지연을 가려내고, 이를 통해 GPU는 거의 최대 처리량에 가까운 효율을 유지할 수 있습니다.

### CUDA 프로그래밍 모델과 하드웨어 매핑

CUDA 프로그래밍 모델은 GPU 하드웨어 구조에 자연스럽게 대응되도록 설계되었습니다. 개발자는 **커널(kernel)** 함수를 작성하며, 이 커널은 다수의 **스레드(thread)**로 실행됩니다. 스레드는 **스레드 블록(thread block)** 단위로 묶이고, 이러한 블록들은 다시 **그리드(grid)**로 구성됩니다.

실행 시점에서 각 스레드 블록은 하나의 **SM(Streaming Multiprocessor)**에 배정되어, 해당 블록의 실행이 끝날 때까지 같은 SM에서만 실행됩니다. 따라서 블록 단위 자원인 **공유 메모리(shared memory)**는 해당 블록의 스레드들만 접근할 수 있습니다. 블록 내부의 스레드들은 공유 메모리를 통해 데이터를 교환하며, `__syncthreads()`와 같은 동기화 함수를 이용해 실행 순서를 맞출 수 있습니다. 블록 자체는 어떤 SM에 배치될지 순서는 보장되지 않지만, **Hopper 아키텍처**에서는 새로운 **스레드 블록 클러스터(thread block cluster)** 개념이 도입되어, 여러 SM에 걸쳐 블록 그룹이 동시에 실행되고 클러스터 단위의 동기화까지 지원할 수 있게 되었습니다. 이를 통해 블록 간 협력이 더 확장된 형태로 가능해졌습니다.

블록 내부의 스레드들은 암묵적으로 **워프(warp)**라는 단위로 묶입니다. 워프는 32개의 스레드로 구성되며, 이는 SM에서 실제 실행되는 기본 단위입니다. 따라서 프로그래머는 보통 블록 크기를 32의 배수로 설정하여 불완전한 워프(partial warp)가 생기지 않도록 합니다. 커널 실행 시 GPU는 전체 스레드를 워프 단위로 나누어 SM에 배치하며, 각 SM은 자원 상황에 따라 다수의 워프를 동시에 실행할 수 있습니다. 최신 아키텍처에서는 SM당 최대 64개의 워프(즉, 2048 스레드)를 동시에 실행할 수 있습니다. 단, 한 블록이 차지하는 레지스터 수나 공유 메모리 크기에 따라 한 SM에 배치될 수 있는 블록의 수가 제한되는데, 이를 **점유율(occupancy)**이라 부릅니다. 효율적인 프로그램은 점유율을 최대화하여 충분한 수의 워프가 실행되도록 조율합니다.

CUDA의 메모리 모델 역시 하드웨어 구조를 반영합니다. **전역 메모리(global memory)**는 GPU의 장치 메모리(HBM)에 해당하며, 대용량이지만 접근 지연(latency)이 수백 나노초 수준으로 비교적 느립니다. 반면 **공유 메모리(shared memory)**는 각 SM 내에 위치한 초고속 온칩 메모리로, 블록 내부 스레드들이 협업에 활용할 수 있습니다. 또한 L1 캐시, 텍스처 캐시, L2 캐시 등이 글로벌 메모리 접근을 암묵적으로 가속합니다. 개발자는 단지 스레드들이 연속적인 주소에 접근하도록(coalescing) 설계해야 메모리 대역폭을 최대한 활용할 수 있습니다. **레지스터**는 스레드 전용 공간으로, 너무 많은 레지스터를 사용할 경우 전체 실행 가능한 스레드 수가 줄어들 수 있습니다. 따라서 CUDA는 레지스터 사용량을 제어할 수 있는 옵션도 제공합니다.

중요한 점은 CUDA 프로그래밍 모델이 **워프 단위 실행**을 내부적으로 추상화한다는 것입니다. 즉, 프로그래머는 각 스레드를 독립적으로 작성하지만, 하드웨어와 컴파일러는 자동으로 32개의 스레드를 워프로 묶어 실행합니다. 그러나 성능 최적화를 위해서는 이 매핑 과정을 이해하는 것이 매우 중요합니다. 예를 들어, 워프 내부에서 스레드들이 서로 다른 분기 경로를 선택하면 **분기 다이버전스(branch divergence)**가 발생하여 직렬 실행으로 이어지므로 성능이 떨어집니다. 또, 워프 내 스레드들이 산발적으로 메모리에 접근하면 **메모리 다이버전스(memory divergence)**로 인해 대역폭 활용도가 낮아집니다. 이러한 문제를 완화하기 위해 CUDA는 **워프 수준 연산자**(예: 워프 셔플, ballot 등)를 제공하며, 숙련된 개발자들은 이를 활용해 워프 내부 협력 알고리즘을 최적화합니다.

결론적으로 CUDA 프로그래밍 모델은 하드웨어 아키텍처와 밀접하게 대응합니다. **그리드와 블록은 SM 계층에 대응되고, 워프는 워프 스케줄러와 SIMT 실행 유닛에 대응되며, CUDA의 다양한 메모리 공간은 실제 GPU의 메모리 계층에 그대로 반영**됩니다. 따라서 CUDA 코드를 작성할 때 이러한 매핑 원리를 이해하는 것이 고성능 GPU 프로그램을 구현하는 열쇠라 할 수 있습니다.

### 딥러닝과 양자화 기능

현대의 NVIDIA GPU 아키텍처는 **딥러닝의 요구**를 크게 반영하여 설계되고 있습니다. 그 핵심은 바로 **Tensor Core**입니다. Tensor Core는 Volta 아키텍처(V100)에서 처음 도입된 **4×4 행렬 곱셈-누적(matrix multiply-accumulate) 유닛**으로, 신경망 학습의 핵심 연산인 **밀집 행렬 곱셈(dense matrix multiplication)**과 **합성곱(convolution)**을 획기적으로 가속화했습니다. V100의 경우, 각 SM은 8개의 Tensor Core를 탑재하여 FP16 행렬 곱셈-누적 연산에서 최대 약 125 TFLOPS를 달성했습니다. 이는 CUDA 코어만 사용할 때의 FP32 연산 성능(약 15 TFLOPS)과 비교했을 때 압도적인 차이였습니다. Tensor Core는 FP16 곱셈과 FP32 누적을 결합한 **혼합 정밀도(mixed precision)** 연산을 지원했으며, 이는 속도와 정밀도를 동시에 만족시켜 딥러닝 학습에 이상적인 환경을 제공했습니다. 또한 NVIDIA는 cuDNN, cuBLAS와 같은 최적화된 라이브러리와 CUDA WMMA API를 제공하여, 개발자가 저수준 어셈블리 코드를 작성하지 않고도 Tensor Core를 쉽게 활용할 수 있도록 했습니다.

**Ampere 아키텍처(A100)**의 3세대 Tensor Core는 데이터 타입 지원과 효율성을 대폭 확장했습니다. 특히 **TensorFloat-32(TF32)** 형식이 도입되어, FP32 입력 범위를 그대로 처리하면서도 FP16 속도로 연산할 수 있게 되었습니다. 이를 통해 기존의 FP32 기반 신경망 모델을 **소스 코드 변경 없이 약 10배 더 빠르게** 실행할 수 있으며, 정확도 손실도 거의 발생하지 않았습니다. 또한 BFloat16 형식을 풀 스피드로 지원하고, **정수 연산(특히 INT8, INT4)**을 강화하여 추론 워크로드에 최적화했습니다. Ampere는 여기에 **구조적 희소성(2:4 sparsity)** 기능을 도입하여, 행렬 내 4개 요소 중 2개가 0일 경우 해당 연산을 생략하도록 했습니다. 그 결과 A100의 Tensor Core는 INT8 추론에서 V100 대비 최대 20배 더 빠른 성능을 낼 수 있었으며, 이는 곧 **양자화 신경망(quantized neural network)**에서 8비트 또는 4비트 정밀도로 효율적인 학습과 추론을 가능하게 했습니다. 실제로 개발자는 TensorRT와 같은 NVIDIA 프레임워크를 이용하여 모델을 자동으로 저정밀도로 변환하고, Tensor Core 기반 하드웨어 가속을 쉽게 활용할 수 있습니다.

**Hopper 아키텍처(H100)**의 4세대 Tensor Core는 이를 한 단계 더 발전시켜 **FP8 정밀도**를 지원합니다. FP8은 메모리 풋프린트를 줄이고 연산 속도를 높이는 데 큰 장점이 있지만, 단순히 8비트를 사용할 경우 정확도 손실이 발생할 수 있습니다. 이를 해결하기 위해 H100은 **Transformer Engine**을 도입했습니다. Transformer Engine은 소프트웨어와 하드웨어가 협력하여 각 레이어별로 FP8 또는 더 높은 정밀도를 동적으로 선택하고, 스케일링을 적용함으로써 대규모 언어 모델 학습과 추론에서 안전하게 FP8 성능을 활용할 수 있도록 합니다. 그 결과 H100은 A100 대비 **학습에서 최대 9배, 추론에서 최대 30배**까지 더 빠른 성능을 달성할 수 있습니다.

Hopper는 또한 AI와 데이터 병렬 처리를 위한 새로운 기능을 추가했습니다. 예를 들어, **DPX 명령어**는 게놈 분석이나 서열 정렬과 같은 동적 프로그래밍 알고리즘을 Ampere 대비 7배 더 빠르게 수행합니다. 또한 **2세대 MIG(Multi-Instance GPU)**와 더 빠른 NVLink를 지원하여, GPU를 7개 인스턴스로 분할할 때에도 대역폭과 QoS가 더 안정적으로 보장됩니다.

CUDA C++ 프로그래밍 관점에서 이러한 기능들은 대부분 **고수준 API와 라이브러리**를 통해 노출됩니다. cuBLAS, cuDNN 같은 라이브러리를 사용하거나 WMMA API를 통해 커널 코드에서 행렬 곱셈을 호출하면 하드웨어가 Tensor Core를 자동으로 사용합니다. INT8/INT4와 같은 양자화나 혼합 정밀도 역시 NVIDIA가 제공하는 도구로 모델을 변환하면 하드웨어가 자동으로 실행합니다. 하지만 연구자 입장에서는 하드웨어 특성을 이해하는 것이 중요합니다. 예를 들어, Ampere의 구조적 희소성을 이해한다면 **2:4 패턴을 활용한 알고리즘 설계**를 할 수 있고, SIMT 실행 특성을 알면 **분기 다이버전스가 심한 레이어(예: LayerNorm, Softmax)**를 최적화할 때 도움이 됩니다.

결론적으로, NVIDIA GPU 아키텍처는 점점 더 **딥러닝과 수치 최적화**에 특화된 방향으로 발전하고 있습니다. 실행 파이프라인(워프, 스케줄러), 전용 하드웨어(Tensor Core), 고대역폭 메모리(HBM) 모두가 유기적으로 결합되어, CUDA C++을 통해 연구자들은 Volta, Ampere, Hopper를 넘어서는 세대에서도 **수십 배의 성능 향상**을 얻을 수 있습니다.
